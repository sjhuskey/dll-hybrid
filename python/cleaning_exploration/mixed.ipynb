{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring HathiTrust Metadata without NLP or AI/ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "author: Samuel J. Huskey\n",
    "'''\n",
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.training.iob_utils import offsets_to_biluo_tags\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSV file with variant names from the DLL Catalog's authority records\n",
    "variant_names = pd.read_csv('input/variant-names.csv')\n",
    "# Load the HathiTrust metadata\n",
    "hathi_raw = pd.read_csv('input/1908698974-1722799169.txt', sep='\\t')\n",
    "# Open the VIAF author data file\n",
    "viaf_data = pd.read_csv('output/viaf-authors-output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedural Python and Pandas Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe with the required columns\n",
    "hathidata = hathi_raw[['author','title','imprint','pub_place','rights_date_used','handle_url']]\n",
    "\n",
    "# Make a VIAF lookup dictionary\n",
    "viaf_lookup = pd.Series(viaf_data.Identifier.values, index=viaf_data['H2 Text']).to_dict()\n",
    "\n",
    "print(f\"Starting with {len(hathidata)} rows.\")\n",
    "\n",
    "# Procedural Python and Pandas operations\n",
    "print(\"Starting procedural Python and Pandas operations.\\n\")\n",
    "\n",
    "# Set the start time\n",
    "start_time = time.time()\n",
    "\n",
    "hathidata.loc[:,'author'] = (\n",
    "    hathidata['author']\n",
    "    .str.strip()        # Trim leading and trailing whitespace\n",
    "    .str.rstrip('.')    # Trim terminal period\n",
    "    .str.rstrip(',')    # Trim terminal comma\n",
    ")\n",
    "\n",
    "# Match as many records as possible without fuzzy matching\n",
    "hathidata.loc[:, 'dll_author_id'] = hathidata.loc[:,'author'].map(viaf_lookup)\n",
    "\n",
    "# Provide the raw list of names as a string\n",
    "names_to_be_omitted = \"\"\"Agathias, d. 582\n",
    "Alciphron\n",
    "Anacreon\n",
    "Apollodorus\n",
    "Apollonius, Dyscolus, 2nd cent\n",
    "Apollonius, Dyscolus, active 2nd century\n",
    "Apollonius, Rhodius\n",
    "Apolodoro de Atenas\n",
    "Archimedes\n",
    "Aristophanes\n",
    "Arrian\n",
    "Arriano, Flavio\n",
    "Artemidoro\n",
    "Athenaeus, of Naucratis\n",
    "Bacchylides\n",
    "Bion, of Phlossa near Smyrna\n",
    "Cassius Dio Cocceianus\n",
    "Cleomedes\n",
    "Constantine VII Porphyrogenitus, Emperor of the East, 905-959\n",
    "Cyril, Saint, Bishop of Jerusalem, approximately 315-386\n",
    "Demosthenes\n",
    "Dio Chrysostomus\n",
    "Diodorus, Siculus\n",
    "Diogenes Laertius\n",
    "Dion Casio\n",
    "Dionisio de Halicarnaso, ca. 60-5 a.C\n",
    "Dionysius Cisterciensis\n",
    "Dionysius, of Halicarnassus\n",
    "Diógenes Laercio\n",
    "Dión Casio\n",
    "Elias, of Nisibis, 975-1046\n",
    "Epictetus\n",
    "Euclid\n",
    "Euclides\n",
    "Euripides\n",
    "Eusebio de Cesarea, Obispo de Cesarea, ca. 265-ca. 340\n",
    "Eusebius, of Caesarea, Bishop of Caesarea, ca. 260-ca. 340.\n",
    "Eustathius, Macrembolites, 12th cent\n",
    "Galen\n",
    "Galeno\n",
    "Gregory, of Nazianzus, Saint\n",
    "Gregory, of Nyssa, Saint, ca. 335-ca. 394\n",
    "Hero of Alexandria\n",
    "Herodian\n",
    "Herodotus\n",
    "Heródoto, 484-425 a. C\n",
    "Hesiod\n",
    "Iamblichus, approximately 250-approximately 330\n",
    "Iamblichus, ca. 250-ca. 330\n",
    "Irenaeus, Saint, Bishop of Lyon\n",
    "Isocrates\n",
    "John Chrysostom, Saint, d. 407\n",
    "John VI Cantacuzenus, Emperor of the East, 1292-1383\n",
    "Juliano, Emperador de Roma, 331-363\n",
    "Justin, Martyr, Saint\n",
    "Justino, Santo, 100?-165?\n",
    "Libanius\n",
    "Lydus, Ioannes Laurentius, 490-\n",
    "Methodius, of Olympus, Saint, -311\n",
    "Michael, of Ephesus\n",
    "Nicander, of Colophon\n",
    "Nicephorus Callistus, ca. 1256-1335\n",
    "Nicephorus, Blemmydes, 1197-1272\n",
    "Orpheus\n",
    "Philo, of Alexandria\n",
    "Pindar\n",
    "Pindarus\n",
    "Plato\n",
    "Platón, ca. 427-348 a.C\n",
    "Plotinus\n",
    "Plutarch\n",
    "Polyaenus\n",
    "Procopius\n",
    "Píndaro, ca. 518-ca. 438 a. C\n",
    "Quintus, Smyrnaeus, 4th cent\n",
    "Sappho\n",
    "Sextus, Empiricus\n",
    "Simplicio\n",
    "Simplicius, of Cilicia\n",
    "Stobaeus\n",
    "Strabo\n",
    "Temistio\n",
    "Teodoreto, Obispo de Ciro\n",
    "Teofrasto\n",
    "Themistius\n",
    "Theocritus\n",
    "Theon, of Smyrna\n",
    "Theophilus, Saint, active 2nd century\n",
    "Theophrastus\n",
    "Thucydides\n",
    "Tryphiodorus\n",
    "Tucídides, ca. 460-ca. 400 a. C\n",
    "Xenophon\n",
    "Xenophon, of Ephesus\n",
    "Yamblico\"\"\"\n",
    "\n",
    "# Split the string by line breaks to create a list\n",
    "names_to_omit_list = names_to_be_omitted.splitlines()\n",
    "\n",
    "# Make a new dataframe without those authors\n",
    "no_greek = hathidata[~hathidata['author'].isin(names_to_omit_list)]\n",
    "\n",
    "# Set the end time\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time in this stage: {end_time - start_time}.\\n\\n\")\n",
    "\n",
    "# Use the count() method to count the number of rows with a value other than \"NaN\"\n",
    "non_nan_count1 = no_greek['dll_author_id'].count()\n",
    "\n",
    "# Print the result to the screen\n",
    "print(f\"There are {len(hathidata) - non_nan_count1} rows without a DLL identifier after straight matching.\\n\")\n",
    "\n",
    "# Make a new dataframe\n",
    "unreconciled = no_greek[no_greek['dll_author_id'].isna()]\n",
    "# Use nunique() to count the unique values in the \"author\" column\n",
    "unreconciled_authors = unreconciled['author'].nunique()\n",
    "\n",
    "print(f\"Out of the original {hathidata['author'].nunique()} authors, {unreconciled_authors} remain unreconciled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original dataframe, filtering out any rows with a value in the dll_author_id field \n",
    "fuzzy = no_greek[no_greek['dll_author_id'].isna()]\n",
    "\n",
    "print(\"Starting fuzzy matching.\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to perform fuzzy matching\n",
    "def fuzzy_match_author(author, lookup_dict, threshold=90):\n",
    "    if pd.isna(author):\n",
    "        return None\n",
    "    match = process.extractOne(author, lookup_dict.keys(), scorer=fuzz.token_sort_ratio)\n",
    "    if match and match[1] >= threshold:\n",
    "        return lookup_dict[match[0]]\n",
    "    return None\n",
    "\n",
    "# Apply the fuzzy_match_author function only to the rows where the mask is True\n",
    "fuzzy.loc[:, 'dll_author_id'] = fuzzy['author'].apply(fuzzy_match_author, args=(viaf_lookup,))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time in this stage: {end_time - start_time}.\\n\")\n",
    "\n",
    "# Use the count() method to count the number of rows with a value other than \"NaN\"\n",
    "non_nan_count2 = fuzzy['dll_author_id'].count()\n",
    "\n",
    "# Print the result to the screen\n",
    "print(f\"There are now {len(hathidata) - (non_nan_count1 + non_nan_count2)} rows out of the original {len(hathidata)} without a DLL identifier after straight and fuzzy matching.\")\n",
    "\n",
    "# Make a new dataframe\n",
    "unreconciled_with_fuzzy = fuzzy[fuzzy['dll_author_id'].isna()]\n",
    "# Use nunique() to count the unique values in the \"author\" column\n",
    "unreconciled_with_fuzzy_authors = unreconciled_with_fuzzy['author'].nunique()\n",
    "\n",
    "print(f\"Out of the {unreconciled_authors} unreconciled authors after the previous process, {unreconciled_with_fuzzy_authors} remain unmatched.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new copy of the dataframe\n",
    "spacyframe = fuzzy[fuzzy['dll_author_id'].isna()]\n",
    "\n",
    "print(\"Starting to apply NER with Spacy.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the trained model\n",
    "nlp = spacy.load(\"custom_ner_model\")\n",
    "\n",
    "# Make a DLL ID lookup dictionary\n",
    "dll_lookup = variant_names[['Authorized Name','DLL Identifier']]\n",
    "dll_lookup = variant_names.set_index('Authorized Name')['DLL Identifier'].to_dict()\n",
    "\n",
    "# Function to identify named entities and match to DLL identifiers\n",
    "def identify_and_match_author(nlp, text, lookup_dict):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            matched_name = ent.text\n",
    "            return lookup_dict.get(matched_name)\n",
    "    return None\n",
    "\n",
    "# Apply the function to the 'author' column and create a new column for DLL identifiers\n",
    "spacyframe.loc[:,'dll_author_id'] = spacyframe['author'].apply(lambda x: identify_and_match_author(nlp, x, dll_lookup))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time in this stage: {end_time - start_time}.\\n\")\n",
    "\n",
    "# Use the count() method to count the number of rows with a value other than \"NaN\"\n",
    "non_nan_count3 = spacyframe['dll_author_id'].count()\n",
    "\n",
    "# Print the result to the screen\n",
    "print(f\"There are now {len(hathidata) - (non_nan_count1 + non_nan_count2 + non_nan_count3)} rows out of the original {len(hathidata)} without a DLL identifier after straight, fuzzy, and NER matching.\")\n",
    "\n",
    "# Make a new dataframe\n",
    "unreconciled_with_spacy = spacyframe[spacyframe['dll_author_id'].isna()]\n",
    "unreconciled_with_spacy_authors = unreconciled_with_spacy['author'].nunique()\n",
    "\n",
    "# Use nunique() to count the unique values in the \"author\" column\n",
    "\n",
    "print(f\"Out of the {unreconciled_with_fuzzy_authors} unreconciled authors after the previous process, {unreconciled_with_spacy_authors} remain unmatched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by author and count unique titles\n",
    "author_title_counts = unreconciled_with_spacy.groupby('author')['title'].nunique()\n",
    "print(\"These are the unreconciled authors and the numbers of rows associated with them:\")\n",
    "# Make sure results aren't truncated\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(author_title_counts)\n",
    "\n",
    "# Filter authors with only one title\n",
    "authors_with_one_title = author_title_counts[author_title_counts == 1]\n",
    "\n",
    "# Count the number of such authors\n",
    "count_of_authors_with_one_title = authors_with_one_title.count()\n",
    "print(f\"\\n\\nOut of {unreconciled_with_spacy_authors} unreconciled authors, {count_of_authors_with_one_title} are 'singletons'.\")\n",
    "print(f\"Percentage of unreconciled authors that are singletons: {count_of_authors_with_one_title/unreconciled_with_spacy_authors}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Whittling\n",
    "\n",
    "There are 4,401 unreconciled authors, and 3,298 of those are apparently \"singletons\". How many of them are actually the same name, just with subtle differences? It's likely that we're dealing with names that don't have an existing record in the DLL Catalog. Otherwise, they would have had a good chance of being matched in the previous processes.\n",
    "\n",
    "I'm going to see how far I can whittle down the list by doing some more fuzzy matching. Below, I'll make a list of the \"unique\" names among the 4,401 unreconciled authors. I'll go line by line, checking to see if the current name closely matches any other name in the list. If it does, I'll put it next to the candidate match, with its probability of being a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to write the results to a CSV.\n",
    "import csv\n",
    "\n",
    "# Print a list of all the unreconciled names\n",
    "unreconciled_with_spacy_list = [name for name in unreconciled_with_spacy['author'].to_list()]\n",
    "unreconciled_with_spacy_set = set(unreconciled_with_spacy_list)\n",
    "filtered_unreconciled_list = [str(name) for name in unreconciled_with_spacy_set]\n",
    "sorted_unreconciled_list = sorted(filtered_unreconciled_list)\n",
    "\n",
    "# Function to find matches for a given name\n",
    "def find_matches(name, names_list, threshold=80):\n",
    "    matches = process.extract(name, names_list, scorer=fuzz.token_sort_ratio, limit=10)\n",
    "    return [(match, score) for match, score, _ in matches if score >= threshold and match != name]\n",
    "\n",
    "# List to store the CSV rows\n",
    "csv_rows = []\n",
    "\n",
    "# Set to keep track of processed names\n",
    "processed_names = set()\n",
    "\n",
    "# Populate the list with matches\n",
    "for name in sorted_unreconciled_list:\n",
    "    if name not in processed_names:\n",
    "        matches = find_matches(name, sorted_unreconciled_list)\n",
    "        if matches:\n",
    "            row = [name]\n",
    "            for match, score in matches:\n",
    "                if match not in processed_names:\n",
    "                    row.extend([match, score])\n",
    "                    processed_names.add(match)\n",
    "            csv_rows.append(row)\n",
    "        processed_names.add(name)\n",
    "\n",
    "# Define the header\n",
    "header = [\"Name 1\", \"Candidate Match 1\", \"Match 1 Probability\", \n",
    "          \"Candidate Match 2\", \"Match 2 Probability\", \n",
    "          \"Candidate Match 3\", \"Match 3 Probability\",\n",
    "          \"Candidate Match 4\", \"Match 4 Probability\",\n",
    "          \"Candidate Match 5\", \"Match 5 Probability\",\n",
    "          \"Candidate Match 6\", \"Match 6 Probability\",\n",
    "          \"Candidate Match 7\", \"Match 7 Probability\",\n",
    "          \"Candidate Match 8\", \"Match 8 Probability\",\n",
    "          \"Candidate Match 9\", \"Match 9 Probability\"]\n",
    "\n",
    "# Write the CSV file\n",
    "with open(\"matched_names.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for row in csv_rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv('matched_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "404 is a lot better than 4,401! It's still a lot, and there are probably some false positives, but this is a good start for creating new authority records to be added to the DLL Catalog.\n",
    "\n",
    "I'll make a final dataframe of the just the reconciled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hathi_id = no_greek[~no_greek['author'].isin(sorted_unreconciled_list)].reset_index()\n",
    "fuzzy_id = fuzzy[~fuzzy['author'].isin(sorted_unreconciled_list)].reset_index()\n",
    "spacy_id = spacyframe[~spacyframe['author'].isin(sorted_unreconciled_list)].reset_index()\n",
    "\n",
    "final_df = pd.concat([hathi_id,fuzzy_id,spacy_id])\n",
    "final_df = final_df[~final_df['dll_author_id'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the 'author' column to build up the list of variant names for authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe of the 'author' and 'dll_author_id' columns\n",
    "author_names = final_df[['author','dll_author_id']]\n",
    "\n",
    "# Open the variant names from VIAF file\n",
    "viaf_names = pd.read_csv('output/viaf-authors-output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns in the VIAF file\n",
    "viaf_names = viaf_names.rename(columns={'H2 Text':'author','Identifier':'dll_author_id'})\n",
    "\n",
    "# Concatenate the name frames\n",
    "names = pd.concat([author_names,viaf_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_deduplicated = names[names.duplicated(subset=['author'], keep=False) == False]\n",
    "print(len(names))\n",
    "print(len(names_deduplicated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_deduplicated.to_csv('output/deep_names.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3228 entries, 0 to 3227\n",
      "Data columns (total 14 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   DLL Identifier                  3227 non-null   object \n",
      " 1   Authorized Name                 3227 non-null   object \n",
      " 2   Short Name                      51 non-null     object \n",
      " 3   Author Name English             1252 non-null   object \n",
      " 4   Author Name Latin               2197 non-null   object \n",
      " 5   Author Name Native Language     1045 non-null   object \n",
      " 6   BNE URL                         562 non-null    object \n",
      " 7   BNF URL                         1361 non-null   object \n",
      " 8   DNB URL                         2068 non-null   object \n",
      " 9   ICCU URL                        407 non-null    object \n",
      " 10  ISNI URL                        1983 non-null   object \n",
      " 11  Other Alternative Name Form(s)  38 non-null     object \n",
      " 12  Perseus Name                    827 non-null    object \n",
      " 13  Wikidata URL                    0 non-null      float64\n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 353.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# I want to add in all the name forms already in the DLL.\n",
    "variant_names = pd.read_csv('input/variant-names.csv')\n",
    "variant_names.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas' melt function to get a listing of variant names matched with a DLL ID.\n",
    "\n",
    "columns_to_melt = [\n",
    "    \"Authorized Name\",\n",
    "    \"Short Name\", \n",
    "    \"Author Name English\", \n",
    "    \"Author Name Latin\", \n",
    "    \"Author Name Native Language\", \n",
    "    \"Other Alternative Name Form(s)\", \n",
    "    \"Perseus Name\"\n",
    "]\n",
    "\n",
    "melted_df = pd.melt(\n",
    "    variant_names, \n",
    "    id_vars=[\"DLL Identifier\"], \n",
    "    value_vars=columns_to_melt,\n",
    "    var_name=\"source_column\", \n",
    "    value_name=\"author\"\n",
    ")\n",
    "\n",
    "# Drop rows with NaN in the \"author\" column\n",
    "melted_df = melted_df.dropna(subset=[\"author\"])\n",
    "\n",
    "# Rename the \"DLL Identifier\" column\n",
    "melted_df = melted_df.rename(columns={\"DLL Identifier\": \"dll_author_id\"})\n",
    "\n",
    "# Select only the relevant columns\n",
    "final_melt = melted_df[[\"author\", \"dll_author_id\"]]\n",
    "\n",
    "# Save to CSV\n",
    "final_melt.to_csv(\"authors_and_identifiers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32043\n",
      "25638\n"
     ]
    }
   ],
   "source": [
    "all_names = pd.concat([names_deduplicated,final_melt])\n",
    "all_names_deduplicated = all_names[all_names.duplicated(subset=['author'], keep=False) == False]\n",
    "\n",
    "print(len(all_names))\n",
    "print(len(all_names_deduplicated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names_deduplicated.to_csv('output/all_names_deduplicated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
